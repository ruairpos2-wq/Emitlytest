#!/usr/bin/env python3
"""
–ü–∞—Ä—Å–µ—Ä –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –†–ë–ö
–ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ —Ä–∞–∑–¥–µ–ª–æ–≤ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤ (–±–µ–∑ –ø—Ä–∏–≤—è–∑–∫–∏ –∫ –°–±–µ—Ä–±–∞–Ω–∫—É)
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
import re
from typing import List, Dict
import xml.etree.ElementTree as ET


class RBCInvestmentsParser:
    """–ü–∞—Ä—Å–µ—Ä –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –†–ë–ö"""
    
    # RSS-–ª–µ–Ω—Ç—ã –¥–ª—è –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π
    RSS_FEEDS = [
        "https://rssexport.rbc.ru/rbcnews/news/30/full.rss",
        "https://rssexport.rbc.ru/rbcnews/business/30/full.rss",
        "https://rssexport.rbc.ru/rbcnews/economics/30/full.rss",
    ]
    
    # –†—É–±—Ä–∏–∫–∏ –¥–ª—è HTML-–ø–∞—Ä—Å–∏–Ω–≥–∞
    HTML_SECTIONS = [
        "https://www.rbc.ru/finances/",
        "https://www.rbc.ru/business/",
        "https://www.rbc.ru/economics/",
        "https://www.rbc.ru/money/",
    ]
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
    INVESTMENT_KEYWORDS = [
        '–∏–Ω–≤–µ—Å—Ç–∏—Ü', '–∞–∫—Ü–∏', '–±–∏—Ä–∂–∞', '—Ñ–æ–Ω–¥', '—Ü–µ–Ω–Ω—ã–µ –±—É–º–∞–≥–∏',
        '–æ–±–ª–∏–≥–∞—Ü', '–¥–∏–≤–∏–¥–µ–Ω–¥', '–∫–∞–ø–∏—Ç–∞–ª', '–ø–æ—Ä—Ç—Ñ–µ–ª—å', '—Ç—Ä–µ–π–¥–∏–Ω–≥',
        '–±—Ä–æ–∫–µ—Ä', 'ipo', '—Ç–æ—Ä–≥–∏', '–∫–æ—Ç–∏—Ä–æ–≤–∫', '–∏–Ω–¥–µ–∫—Å', '—Ä—ã–Ω–æ–∫',
        '–≤–∞–ª—é—Ç', '–∑–æ–ª–æ—Ç–æ', '–Ω–µ—Ñ—Ç—å', '–≥–∞–∑', '–º–µ—Ç–∞–ª–ª'
    ]
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
        }
        self.cutoff_date = datetime.now() - timedelta(days=7)  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π
        print(f"–ò—â–µ–º –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –Ω–∞—á–∏–Ω–∞—è —Å: {self.cutoff_date.strftime('%Y-%m-%d')}")
    
    def _is_investment_news(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –ª–∏ –Ω–æ–≤–æ—Å—Ç—å –∫ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏—è–º"""
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in self.INVESTMENT_KEYWORDS)
    
    def _parse_date(self, date_str: str) -> datetime:
        """–ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤"""
        try:
            # RFC 2822 format (RSS)
            from email.utils import parsedate_to_datetime
            dt = parsedate_to_datetime(date_str)
            # –£–±–∏—Ä–∞–µ–º timezone info –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            return dt.replace(tzinfo=None)
        except:
            try:
                # ISO format
                dt = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                return dt.replace(tzinfo=None)
            except:
                return datetime.now()
    
    def parse_rss_feeds(self) -> List[Dict]:
        """–ü–∞—Ä—Å–∏—Ç RSS-–ª–µ–Ω—Ç—ã –†–ë–ö"""
        results = []
        
        for feed_url in self.RSS_FEEDS:
            print(f"\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ RSS: {feed_url}")
            
            try:
                response = requests.get(feed_url, headers=self.headers, timeout=10)
                
                if response.status_code == 404:
                    print(f"  ‚ö† RSS –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (404)")
                    continue
                    
                response.raise_for_status()
                
                # –ü–∞—Ä—Å–∏–º XML
                root = ET.fromstring(response.content)
                
                items_count = 0
                found_count = 0
                
                # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã item –≤ RSS
                for item in root.findall('.//item'):
                    items_count += 1
                    
                    title_elem = item.find('title')
                    link_elem = item.find('link')
                    pub_date_elem = item.find('pubDate')
                    description_elem = item.find('description')
                    
                    if title_elem is None or link_elem is None:
                        continue
                    
                    title = title_elem.text
                    url = link_elem.text
                    description = description_elem.text if description_elem is not None else ""
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞—Ç—É
                    if pub_date_elem is not None:
                        pub_date = self._parse_date(pub_date_elem.text)
                        if pub_date < self.cutoff_date:
                            continue
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –ª–∏ –∫ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏—è–º
                    full_text = f"{title} {description}"
                    if self._is_investment_news(full_text):
                        date_str = pub_date.strftime('%Y-%m-%d') if pub_date_elem else datetime.now().strftime('%Y-%m-%d')
                        
                        results.append({
                            'title': title,
                            'url': url,
                            'date': date_str,
                            'source': 'RSS'
                        })
                        found_count += 1
                        print(f"  ‚úì [{date_str}] {title[:60]}...")
                
                print(f"  –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ: {items_count} –Ω–æ–≤–æ—Å—Ç–µ–π, –Ω–∞–π–¥–µ–Ω–æ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã—Ö: {found_count}")
                time.sleep(1)
                
            except requests.exceptions.Timeout:
                print(f"  ‚úó –¢–∞–π–º–∞—É—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è")
            except requests.exceptions.RequestException as e:
                print(f"  ‚úó –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            except ET.ParseError as e:
                print(f"  ‚úó –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ XML: {e}")
        
        return results
    
    def parse_html_sections(self, max_pages: int = 3) -> List[Dict]:
        """–ü–∞—Ä—Å–∏—Ç HTML-—Ä–∞–∑–¥–µ–ª—ã –†–ë–ö"""
        results = []
        
        for section_url in self.HTML_SECTIONS:
            print(f"\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–¥–µ–ª–∞: {section_url}")
            
            try:
                response = requests.get(section_url, headers=self.headers, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # –ò—â–µ–º —Å—Ç–∞—Ç—å–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                articles = soup.find_all('a', class_=re.compile(r'(item__link|news-feed__item)'))
                
                found_count = 0
                
                for article in articles[:30]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 30 —Å—Ç–∞—Ç–µ–π
                    title = article.get_text(strip=True)
                    url = article.get('href', '')
                    
                    if not url.startswith('http'):
                        url = f"https://www.rbc.ru{url}"
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –ª–∏ –∫ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏—è–º
                    if self._is_investment_news(title):
                        results.append({
                            'title': title,
                            'url': url,
                            'date': datetime.now().strftime('%Y-%m-%d'),
                            'source': 'HTML'
                        })
                        found_count += 1
                        print(f"  ‚úì {title[:60]}...")
                
                print(f"  –ù–∞–π–¥–µ–Ω–æ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {found_count}")
                time.sleep(2)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                
            except requests.exceptions.RequestException as e:
                print(f"  ‚úó –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        
        return results
    
    def parse(self) -> List[Dict]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        print("\n" + "="*70)
        print("–ü–ê–†–°–ò–ù–ì –ò–ù–í–ï–°–¢–ò–¶–ò–û–ù–ù–´–• –ù–û–í–û–°–¢–ï–ô –†–ë–ö")
        print("="*70)
        
        # RSS –ø–∞—Ä—Å–∏–Ω–≥
        rss_results = self.parse_rss_feeds()
        print(f"\nüì∞ RSS: –Ω–∞–π–¥–µ–Ω–æ {len(rss_results)} –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
        
        # HTML –ø–∞—Ä—Å–∏–Ω–≥
        html_results = self.parse_html_sections()
        print(f"\nüåê HTML: –Ω–∞–π–¥–µ–Ω–æ {len(html_results)} –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        all_results = rss_results + html_results
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
        unique_results = []
        seen_urls = set()
        
        for result in all_results:
            if result['url'] not in seen_urls:
                unique_results.append(result)
                seen_urls.add(result['url'])
        
        print(f"\n‚úÖ –ò—Ç–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {len(unique_results)}")
        
        return unique_results


def main():
    """–¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞"""
    parser = RBCInvestmentsParser()
    results = parser.parse()
    
    print("\n" + "="*70)
    print("–ü–†–ò–ú–ï–†–´ –ù–ê–ô–î–ï–ù–ù–´–• –ù–û–í–û–°–¢–ï–ô")
    print("="*70)
    
    for i, news in enumerate(results[:5], 1):
        print(f"\n{i}. [{news['date']}] {news['title']}")
        print(f"   {news['url']}")


if __name__ == '__main__':
    main()
